{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/ \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from rdkit import Chem\n",
    "from mordred import Calculator, descriptors\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "matplotlib.rcParams.update({'font.size': 24})\n",
    "PATH_DATA = \"../datasets/CYP/\"\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L model DNN\n",
    "\n",
    "Generate DNN with L layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(layers_dim, lr, dropout, optimizer, L2):\n",
    "    \"\"\"layers_dim -- [n_input, n_hid_1, ..., n_output=1]\"\"\"\n",
    "    hidden_layers = []\n",
    "    for i in range(1,len(layers_dim)-1): hidden_layers.extend([tf.keras.layers.Dropout(dropout)] + [tf.keras.layers.Dense(layers_dim[i], activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(L2))])\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(layers_dim[0], activation='relu', input_shape=(layers_dim[0],))] +\n",
    "        hidden_layers + \n",
    "        [tf.keras.layers.Dense(layers_dim[-1], activation=\"sigmoid\")])\n",
    "    loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are useful to stop learning when some condition is reached (among other things I guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.90):\n",
    "          print(\"\\n Reached 80% accuracy on training set so cancelling training!\")\n",
    "          self.model.stop_training = True\n",
    "        if(logs.get('val_accuracy')>0.80):\n",
    "          print(\"\\n Reached 70% accuracy on validation set so cancelling training!\")\n",
    "          self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using DNN of L layers with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dim = [train_data.shape[1], 15, 5, 1]\n",
    "lr = 0.001\n",
    "dropout = 0.2\n",
    "optimizer = 'RMSprop'\n",
    "L2 = 0.001\n",
    "\n",
    "model_small_dataset =generate_model(layers_dim, lr, dropout, optimizer, L2)\n",
    "#model_small_dataset.summary()\n",
    "\n",
    "history = model_small_dataset.fit(\n",
    "      train_data,train_labels,\n",
    "      epochs=50,\n",
    "      verbose=2,\n",
    "      #validation_data = (mini_testing_2c9_data, mini_labels_testing_2c9_data)\n",
    "      validation_data = (test_data, test_labels)\n",
    "      )\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"../datasets/CYP/\"\n",
    "\n",
    "shared_data = pd.read_csv(os.path.join(PATH_DATA, \"shared_set_cyp.csv\"))\n",
    "labels_2c9 = (shared_data[\"p450-cyp2c9 Activity Outcome\"] == \"Active\").values.astype(int)\n",
    "labels_3a4 = (shared_data[\"p450-cyp3a4 Activity Outcome\"] == \"Active\").values.astype(int)\n",
    "testing_2c9_data = pd.read_csv(os.path.join(PATH_DATA, \"only_2c9_set_cyp.csv\"))\n",
    "labels_testing_2c9 = (testing_2c9_data[\"p450-cyp2c9 Activity Outcome\"] == \"Active\").values.astype(int)\n",
    "testing_3a4_data = pd.read_csv(os.path.join(PATH_DATA, \"only_3a4_set_cyp.csv\"))\n",
    "labels_testing_3a4 = (testing_3a4_data[\"p450-cyp3a4 Activity Outcome\"] == \"Active\").values.astype(int)\n",
    "\n",
    "#features_shared = np.load(os.path.join(\"features\", \"shared_set_features.npy\"))\n",
    "#features_only_2c9 = np.load(os.path.join(\"features\", \"only_2c9_set_features.npy\"))\n",
    "#features_only_3a4 = np.load(os.path.join(\"features\", \"only_3a4_set_features.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting features (fingerprints)\n",
    "\n",
    "There are many [fingerprints](http://rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf):\n",
    "\n",
    "- Morgan\n",
    "- MACCS\n",
    "- RDKit fingerprint (to do)\n",
    "\n",
    "Posar link paper que compara fingerprints (Dani)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(input_sdf):\n",
    "    structures_shared = Chem.SDMolSupplier(input_sdf)\n",
    "    features = []\n",
    "    for mol in structures_shared:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol,2,nBits=1024)\n",
    "        arr = np.zeros((0,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(fp,arr)\n",
    "        features.append(arr)\n",
    "    return np.array(features)\n",
    "\n",
    "if os.path.exists(os.path.join(\"features\", \"shared_set_features.npy\")):\n",
    "    features_shared = np.load(os.path.join(\"features\", \"shared_set_features.npy\"))\n",
    "else:\n",
    "    features_shared = get_features(os.path.join(PATH_DATA, \"shared_set_cyp.sdf\"))\n",
    "    np.save(os.path.join(\"features\", \"shared_set_features.npy\"), features_shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MACCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(input_sdf):\n",
    "    structures_shared = Chem.SDMolSupplier(input_sdf)\n",
    "    features = []\n",
    "    for mol in structures_shared:\n",
    "        fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        arr = np.zeros((0,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(fp,arr)\n",
    "        features.append(arr)\n",
    "    return np.array(features)\n",
    "\n",
    "if os.path.exists(os.path.join(\"features\", \"shared_set_features_MACCS.npy\")):\n",
    "    features_shared = np.load(os.path.join(\"features\", \"shared_set_features_MACCS.npy\"))\n",
    "else:\n",
    "    features_shared = get_features(os.path.join(PATH_DATA, \"shared_set_cyp.sdf\"))\n",
    "    np.save(os.path.join(\"features\", \"shared_set_features_MACCS.npy\"), features_shared)\n",
    "    \n",
    "if os.path.exists(os.path.join(\"features\", \"only_2c9_set_features_MACCS.npy\")):\n",
    "    features_only_2c9 = np.load(os.path.join(\"features\", \"only_2c9_set_features_MACCS.npy\"))\n",
    "else:   \n",
    "    features_only_2c9 = get_features(os.path.join(PATH_DATA, \"only_2c9_set_cyp.sdf\"))\n",
    "    np.save(os.path.join(\"features\", \"only_2c9_set_features_MACCS.npy\"), features_only_2c9)\n",
    "    \n",
    "if os.path.exists(os.path.join(\"features\", \"only_3a4_set_features_MACCS.npy\")):\n",
    "    features_only_3a4 = np.load(os.path.join(\"features\", \"only_3a4_set_features_MACCS.npy\"))\n",
    "else:   \n",
    "    features_only_3a4 = get_features(os.path.join(PATH_DATA, \"only_3a4_set_cyp.sdf\"))\n",
    "    np.save(os.path.join(\"features\", \"only_3a4_set_features_MACCS.npy\"), features_only_3a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(input_sdf):\n",
    "    structures_shared = Chem.SDMolSupplier(input_sdf)\n",
    "    features = []\n",
    "    for mol in structures_shared:\n",
    "        fp = Chem.RDKFingerprint(mol)\n",
    "        arr = np.zeros((0,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(fp,arr)\n",
    "        features.append(arr)\n",
    "    return np.array(features)\n",
    "\n",
    "if os.path.exists(os.path.join(\"features\", \"shared_set_features_RDKIT.npy\")):\n",
    "    features_shared = np.load(os.path.join(\"features\", \"shared_set_features_RDKIT.npy\"))\n",
    "else:\n",
    "    features_shared = get_features(os.path.join(PATH_DATA, \"shared_set_cyp.sdf\"))\n",
    "    np.save(os.path.join(\"features\", \"shared_set_features_RDKIT.npy\"), features_shared)\n",
    "    \n",
    "if os.path.exists(os.path.join(\"features\", \"only_2c9_set_features_RDKIT.npy\")):\n",
    "    features_only_2c9 = np.load(os.path.join(\"features\", \"only_2c9_set_features_RDKIT.npy\"))\n",
    "else:   \n",
    "    features_only_2c9 = get_features(os.path.join(PATH_DATA, \"only_2c9_set_cyp.sdf\"))\n",
    "    np.save(os.path.join(\"features\", \"only_2c9_set_features_RDKIT.npy\"), features_only_2c9)\n",
    "    \n",
    "if os.path.exists(os.path.join(\"features\", \"only_3a4_set_features_RDKIT.npy\")):\n",
    "    features_only_3a4 = np.load(os.path.join(\"features\", \"only_3a4_set_features_RDKIT.npy\"))\n",
    "else:   \n",
    "    features_only_3a4 = get_features(os.path.join(PATH_DATA, \"only_3a4_set_cyp.sdf\"))\n",
    "    np.save(os.path.join(\"features\", \"only_3a4_set_features_RDKIT.npy\"), features_only_3a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting physicochemical descriptors\n",
    "\n",
    "Molecular descriptors are widely employed to present molecular characteristics in cheminformatics. Various molecular-descriptor-calculation software programs have been developed.\n",
    "\n",
    "- [Mordred](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0258-y) (To do)\n",
    "- PaDEL - Descriptor\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mordred import Calculator, descriptors\n",
    "\n",
    "# create descriptor calculator with all descriptors\n",
    "calc = Calculator(descriptors, ignore_3D=True)\n",
    "#len(calc.descriptors)\n",
    "\n",
    "class_col = shared_data['p450-cyp2c9 Activity Outcome']\n",
    "class_arr = class_col.to_numpy()\n",
    "class_arr = np.reshape(class_arr, [class_arr.shape[0],1])\n",
    "class_arr = np.squeeze(class_arr)\n",
    "smi_col = shared_data['CanonicalSMILES']\n",
    "smi_arr = smi_col.to_numpy()\n",
    "smi_arr = np.reshape(smi_arr, [smi_arr.shape[0],1])\n",
    "smi_arr = np.squeeze(smi_arr)\n",
    "\n",
    "# calculate descriptors for a single molecule using smile\n",
    "mol = Chem.MolFromSmiles(smi_arr[300])\n",
    "calc(mol)[:3]\n",
    "\n",
    "# get descriptors in a df\n",
    "df_descriptors = calc.pandas(mols_short)\n",
    "#df_descriptors['SLogP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting and removing outliers\n",
    "\n",
    "Before selecting features, we will remove the examples that contain outliers using the Z score criteria.\n",
    "\n",
    "$Z = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "Z-scores can quantify the unusualness of an observation when your data follow the normal distribution. Z-scores are the number of standard deviations above and below the mean that each value falls. For example, a Z-score of 2 indicates that an observation is two standard deviations above the average while a Z-score of -2 signifies it is two standard deviations below the mean. A Z-score of zero represents a value that equals the mean. [Source](https://statisticsbyjim.com/basics/outliers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_z_score(df_original):\n",
    "    df=df_original.copy()\n",
    "    headers = []\n",
    "    for col in df.columns:\n",
    "        df[f'{col}_zscore'] = (df[col] - df[col].mean())/df[col].std(ddof=0)\n",
    "        headers.append(col)\n",
    "    return df, headers\n",
    "\n",
    "def outliers_detection(df, threshold=3):\n",
    "    df_scored, headers=compute_z_score(df)\n",
    "    zscore_col = list(set(df_scored.columns) - set(headers)) # to only evaluate zscore columns\n",
    "    for col in zscore_col:\n",
    "        df_scored[f'{col}_outlier'] = (abs(df_scored[f'{col}'])> threshold).astype(int)\n",
    "    return df_scored, zscore_col\n",
    "\n",
    "def drop_outliers(df, threshold=3):\n",
    "    df_outlier, zscore_col = outliers_detection(df, threshold=threshold)\n",
    "    for col in zscore_col:\n",
    "        index = df_outlier[ df_outlier[f'{col}_outlier'] == 1 ].index\n",
    "        df_outlier.drop(index , inplace=True)\n",
    "        df_outlier.drop(col , inplace=True,axis = 1)\n",
    "        df_outlier.drop(f'{col}_outlier' , inplace=True, axis = 1)\n",
    "    return df_outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !! The following function should work but it doesn't. \n",
    "\n",
    "It is more elegant that the one above. Moreover, I think it could be more efficient.\n",
    "\n",
    "The error that I get is:\n",
    "\n",
    "*RuntimeWarning: invalid value encountered in less\n",
    "This is separate from the ipykernel package so we can avoid doing imports until*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(df_original, threshold=3):\n",
    "    df=df_original.copy()\n",
    "    return df[(np.abs(stats.zscore(df)) < threshold).all(axis=1)]\n",
    "    \n",
    "df_2 = drop_outliers(descriptors_only2c9.astype('float64'),threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptors normalisation \n",
    "\n",
    "Here we have an example of how to normalise with Skit learn `normalize()` function.\n",
    "\n",
    "Each column (containig a single descriptor) is divided by the maximum value.\n",
    "\n",
    "This step should be always carried out after removing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_descriptors_shared = pd.DataFrame(normalize(descriptors_shared, norm='max', axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ferature selection\n",
    "\n",
    "There are mainly [two ways](https://machinelearningmastery.com/feature-selection-with-categorical-data/) of chossing features of a dataset taking into account it's labels (`select_features()`):\n",
    "\n",
    "- [Chi square](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) (`chi2`): [Pearson test](https://stackoverflow.com/questions/25792012/feature-selection-using-scikit-learn) is only valid for positive values.\n",
    "- [Mutual info classification](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) (`mutual_info_classif`)\n",
    "\n",
    "Also we can remove all features whose variance doesn’t meet some threshold, independently of how are they classified.\n",
    "By default, it removes all zero-variance features, i.e. features that have the same value in all samples (`select_features_threshold`):\n",
    "\n",
    "- [Removing features with low variance](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection): The smaller threshold ($p$), the more restrictive is the function, since the smaller will be the variance threshold:\n",
    "\n",
    "    $Var_{threshold} =  p(1-p)<Var_{x} \\rightarrow Accepted$\n",
    "\n",
    "We can combine both ways to select features `select_features_comb()`:\n",
    "- First we will select the features taking into account the labels of the data, using `select_features()`.\n",
    "- Then we will set a variance threshold to the selected features with `select_features_threshold()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_univar(X_train, Y_train, X_test, score_func=chi2, k_best=None, percentile = None):\n",
    "    \"\"\"score_func=chi2 (default), mutual_info_classif\"\"\"\n",
    "    if not k_best == None:\n",
    "        fs = SelectKBest(score_func=score_func, k=k_best)\n",
    "    elif not percentile == None:\n",
    "        fs = SelectPercentile(score_func=score_func, percentile=percentile)\n",
    "    else:\n",
    "        print(\"Introduce the number of best features to be kept (`k_best`) or the percentil.\")\n",
    "        return\n",
    "    fs.fit(X_train, Y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "    \n",
    "def select_features_threshold(X_train, X_test, threshold=0.9):\n",
    "    \"\"\"It removes all features whose variance doesn’t meet some threshold. It doesn't take into account the \"\"\"\n",
    "    sel = VarianceThreshold(threshold=(threshold * (1 - threshold)))\n",
    "    X_train_threshold = sel.fit_transform(X_train)\n",
    "    X_test_threshold = sel.transform(X_test)\n",
    "    return X_train_threshold, X_test_threshold\n",
    "\n",
    "def select_features_comb(X_train, Y_train, X_test, score_func=chi2, k_best=None, percentile = None, threshold=0.9):\n",
    "    X_train_fs, X_test_fs, _ = select_features_univar(X_train, Y_train, X_test, score_func=chi2, k_best=k_best, percentile = percentile)\n",
    "    X_train_comb, X_test_comb = select_features_threshold(X_train_fs, X_test_fs, threshold=threshold)\n",
    "    return X_train_comb, X_test_comb\n",
    "\n",
    "def plot_score(fs, print_scores=False):\n",
    "    \"\"\"plot the score for all the features\"\"\"\n",
    "    if print_scores:\n",
    "        for i in range(len(fs.scores_)):\n",
    "            print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "    plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tunning with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows the exploration of the hyperparameters using Tensorboard.\n",
    "The hyperparameter that are explored in a L layer NN with dropout at the first hidden layer are:\n",
    "\n",
    "- Number of hidden layers \n",
    "- Number of neurons for the hidden layer (all the hidden layers in the NN will have the same number of neurons).\n",
    "- Dropout (to avoid overfitting)\n",
    "- Optimizer\n",
    "- L2 regularizer (to avoid overfitting)\n",
    "- Learning rate (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_tunning = True\n",
    "\n",
    "HP_HIDDEN_LAYERS = hp.HParam(\"hidden_layers\", hp.Discrete(list(range(3, 10))))\n",
    "HP_NEURONS = hp.HParam(\"neurons\", hp.Discrete([i for i in range(10,151,20)]))\n",
    "HP_DROPOUT = hp.HParam(\"dropout\", hp.RealInterval(0.2, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd','RMSprop']))\n",
    "HP_L2 = hp.HParam('l2 regularizer', hp.RealInterval(.001,.01))\n",
    "HP_LR = hp.HParam(\"learning_rate\", hp.Discrete([0.001, 0.01, 0.1, 1.0, 10.0]))\n",
    "N_BITS = train_data.shape[1]\n",
    "    \n",
    "def construct_optimizer(hparams):\n",
    "    if hparams[HP_OPTIMIZER] == \"adam\":\n",
    "        return tf.keras.optimizers.Adam(learning_rate = hparams[HP_LR])\n",
    "    elif hparams[HP_OPTIMIZER] == \"sgd\":\n",
    "        return tf.keras.optimizers.SGD(learning_rate = hparams[HP_LR])\n",
    "    elif hparams[HP_OPTIMIZER] == \"RMSprop\":\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate = hparams[HP_LR])\n",
    "    \n",
    "def train_test_model(hparams):\n",
    "    internal_layers = [tf.keras.layers.Dropout(hparams[HP_DROPOUT])]+[tf.keras.layers.Dense(hparams[HP_NEURONS], kernel_regularizer=tf.keras.regularizers.l2(hparams[HP_L2]), activation='relu') for _ in range(hparams[HP_HIDDEN_LAYERS])]\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(N_BITS, activation='relu', input_shape=(N_BITS,))]+\n",
    "        internal_layers+[tf.keras.layers.Dense(1, activation=\"sigmoid\")]\n",
    "    )\n",
    "    model.compile(optimizer=construct_optimizer(hparams), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "    model.fit(train_data, train_labels, epochs=10, verbose=2, class_weight=class_weight)\n",
    "    _, results = model.evaluate(val_data, val_labels, verbose=0)                                                                                                \n",
    "    _, results_val = model.evaluate(mini_testing_2c9_data, mini_labels_testing_2c9_data, verbose=0)\n",
    "    return results, results_val \n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy, accuracy_val = train_test_model(hparams)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy, step=1)\n",
    "    tf.summary.scalar(\"accuracy_val\", accuracy_val, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"hyperparameters_tunning/morgan_mordred_norm_6060_feature_selection\", exist_ok=True)\n",
    "with tf.summary.create_file_writer('hyperparameters_tunning/morgan_mordred_norm_6060_feature_selection').as_default():\n",
    "    hp.hparams_config(hparams=[HP_HIDDEN_LAYERS,HP_NEURONS, HP_DROPOUT, HP_OPTIMIZER, HP_L2, HP_LR],\n",
    "                      metrics=[hp.Metric(\"accuracy\", display_name='Accuracy'), hp.Metric(\"accuracy_val\", display_name=\"Validation_accuracy\")]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir hyperparameters_tunning/morgan_mordred_norm_6060_feature_selection/logs/hparam_tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparam_tunning:\n",
    "    session_num = 0\n",
    "    looping = list(product(HP_NEURONS.domain.values, HP_HIDDEN_LAYERS.domain.values,[HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value], HP_OPTIMIZER.domain.values, [HP_L2.domain.min_value, HP_L2.domain.max_value], HP_LR.domain.values))\n",
    "    random.shuffle(looping)\n",
    "    #looping = product(HP_NEURONS.domain.values, HP_HIDDEN_LAYERS.domain.values, [HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value], HP_OPTIMIZER.domain.values, [HP_L2.domain.min_value, HP_L2.domain.max_value], HP_LR.domain.values)\n",
    "    total_runs = len(list(looping))\n",
    "    for neurons, hidden_lay, dropout, opt, l2, lr in looping:\n",
    "        hp_params = {HP_NEURONS: neurons, HP_HIDDEN_LAYERS: hidden_lay, HP_DROPOUT: dropout, HP_OPTIMIZER: opt, HP_L2: l2, HP_LR: lr}\n",
    "        if session_num % 10 == 0:\n",
    "            # clear everything every 10 models to avoid oom errors\n",
    "            tf.keras.backend.clear_session()\n",
    "        run_name = f\"run_{session_num}\"\n",
    "        print(f\"---Starting trial: {run_name} of {total_runs}\")\n",
    "        print({h.name: hp_params[h] for h in hp_params})\n",
    "        run('hyperparameters_tunning/morgan_mordred_norm_6060_feature_selection/logs/hparam_tuning/' + run_name, hp_params)\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### !! The following version does not work by the moment... Should be fixed\n",
    "\n",
    "This code allows the exploration of the number of neurons for a prefix number of layer (-> the function `\n",
    "train_test_model` has to be changed adding or removing layers...)\n",
    "\n",
    "- It used to fail with sgd optimizer, that's why adagrad is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HP_HIDDEN_LAYERS = hp.HParam(\"hidden_layers\", hp.Discrete(list(range(2, 6))))\n",
    "#per ara sense hidden layers com a hyperparametre. Primer mirarem com fer-ho amb diferents capes.\n",
    "HP_NEURONS_1 = hp.HParam(\"neurons_1\", hp.Discrete(list(range(5, 66, 5))))\n",
    "HP_NEURONS_2 = hp.HParam(\"neurons_2\", hp.Discrete(list(range(5, 66, 5))))\n",
    "HP_NEURONS_3 = hp.HParam(\"neurons_3\", hp.Discrete(list(range(5, 66, 5))))\n",
    "HP_NEURONS_4 = hp.HParam(\"neurons_4\", hp.Discrete(list(range(5, 66, 5))))\n",
    "HP_NEURONS_5 = hp.HParam(\"neurons_5\", hp.Discrete(list(range(5, 66, 5))))\n",
    "\n",
    "HP_DROPOUT = hp.HParam(\"dropout\", hp.RealInterval(0.2, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'adagrad','RMSprop'])) # 'sgd'\n",
    "HP_L2 = hp.HParam('l2_regularizer', hp.RealInterval(.001,.01))\n",
    "HP_LR = hp.HParam(\"learning_rate\", hp.Discrete([0.0001, 0.001, 0.01, 0.1, 1.0]))\n",
    "\n",
    "os.makedirs(\"hyperparameters_tunning/morgan_mordred_feature_selection\", exist_ok=True)\n",
    "with tf.summary.create_file_writer('hyperparameters_tunning/morgan_mordred_feature_selection/logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(hparams=[HP_NEURONS_1, HP_NEURONS_2, HP_DROPOUT, HP_OPTIMIZER, HP_L2, HP_LR], # 2 hid_layers by the moment\n",
    "                      metrics=[hp.Metric(\"accuracy\", display_name='Accuracy')])#, hp.Metric('precision', display_name='Precision'),hp.Metric('recall', display_name='Recall')])\n",
    "    \n",
    "def construct_optimizer(hparams):\n",
    "    if hparams[HP_OPTIMIZER] == \"adam\":\n",
    "        return tf.keras.optimizers.Adam(learning_rate = hparams[HP_LR])\n",
    "    elif hparams[HP_OPTIMIZER] == \"adagrad\":\n",
    "        return tf.keras.optimizers.Adagrad(learning_rate = hparams[HP_LR])\n",
    "    #elif hparams[HP_OPTIMIZER] == \"sgd\":\n",
    "    #    return tf.keras.optimizers.SGD(learning_rate = hparams[HP_LR])\n",
    "    elif hparams[HP_OPTIMIZER] == \"RMSprop\":\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate = hparams[HP_LR])\n",
    "    \n",
    "def train_test_model(hparams):\n",
    "    internal_layers = [tf.keras.layers.Dropout(hparams[HP_DROPOUT]), \n",
    "                       tf.keras.layers.Dense(hparams[HP_NEURONS_1], kernel_regularizer=tf.keras.regularizers.l2(hparams[HP_L2]), activation='relu'),\n",
    "                       tf.keras.layers.Dropout(hparams[HP_DROPOUT]), \n",
    "                       tf.keras.layers.Dense(hparams[HP_NEURONS_2], kernel_regularizer=tf.keras.regularizers.l2(hparams[HP_L2]), activation='relu'),\n",
    "                       tf.keras.layers.Dropout(hparams[HP_DROPOUT]), \n",
    "                       tf.keras.layers.Dense(hparams[HP_NEURONS_3], kernel_regularizer=tf.keras.regularizers.l2(hparams[HP_L2]), activation='relu')\n",
    "                      ]\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(train_data.shape[1], activation='relu', input_shape=(train_data.shape[1],))] +\n",
    "        internal_layers + [tf.keras.layers.Dense(1, activation=\"sigmoid\")]\n",
    "    )\n",
    "    model.compile(optimizer=construct_optimizer(hparams), loss=\"binary_crossentropy\", metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    model.fit(train_data, train_labels, epochs=5, verbose=2)\n",
    "    results = model.evaluate(val_data, val_labels)\n",
    "    #_, results = model.evaluate(val_data, val_labels)\n",
    "    return results\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "      if os.path.exists(run_dir):\n",
    "        return\n",
    "      with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_tunning =True\n",
    "\n",
    "if hyperparam_tunning:\n",
    "    session_num = 0\n",
    "    total_runs=5\n",
    "    #total_runs = len(HP_NEURONS_1.domain.values)*len(HP_NEURONS_2.domain.values)*len([HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value])*len(HP_OPTIMIZER.domain.values)*len([HP_L2.domain.min_value, HP_L2.domain.max_value])*len(HP_LR.domain.values)\n",
    "    looping = list(product(HP_NEURONS_1.domain.values, HP_NEURONS_2.domain.values, HP_NEURONS_3.domain.values,[HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value], HP_OPTIMIZER.domain.values, [HP_L2.domain.min_value, HP_L2.domain.max_value], HP_LR.domain.values))\n",
    "    random.shuffle(looping)\n",
    "    for neurons_1, neurons_2, neurons_3, dropout, opt, l2, lr in looping:\n",
    "        hp_params = {HP_NEURONS_1: neurons_1, HP_NEURONS_2: neurons_2, HP_NEURONS_3: neurons_3, HP_DROPOUT: dropout, HP_OPTIMIZER: opt, HP_L2: l2, HP_LR: lr}\n",
    "        if session_num % 10 == 0:\n",
    "            # clear everything every 10 models to avoid oom errors\n",
    "            tf.keras.backend.clear_session()\n",
    "        run_name = f\"run_{session_num}\"\n",
    "        print(f\"---Starting trial: {run_name} of {total_runs}\")\n",
    "        print({h.name: hp_params[h] for h in hp_params})\n",
    "        run('hyperparameters_tunning/morgan_mordred_feature_selection/logs/hparam_tuning/' + run_name, hp_params)\n",
    "        session_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
