{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/ \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "plt.style.use(\"ggplot\")\n",
    "matplotlib.rcParams.update({'font.size': 24})\n",
    "PATH_DATA = \"../datasets/CYP/\"\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L model DNN\n",
    "\n",
    "Generate DNN with L layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(layers_dim, lr, dropout, optimizer, L2):\n",
    "    \"\"\"layers_dim -- [n_input, n_hid_1, ..., n_output=1]\"\"\"\n",
    "    hidden_layers = []\n",
    "    for i in range(1,len(layers_dim)-1): hidden_layers.extend([tf.keras.layers.Dropout(dropout)] + [tf.keras.layers.Dense(layers_dim[i], activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(L2))])\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(layers_dim[0], activation='relu', input_shape=(layers_dim[0],))] +\n",
    "        hidden_layers + \n",
    "        [tf.keras.layers.Dense(layers_dim[-1], activation=\"sigmoid\")])\n",
    "    loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are useful to stop learning when some condition is reached (among other things I guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.90):\n",
    "          print(\"\\n Reached 80% accuracy on training set so cancelling training!\")\n",
    "          self.model.stop_training = True\n",
    "        if(logs.get('val_accuracy')>0.80):\n",
    "          print(\"\\n Reached 70% accuracy on validation set so cancelling training!\")\n",
    "          self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using DNN of L layers with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dim = [train_data.shape[1], 15, 5, 1]\n",
    "lr = 0.001\n",
    "dropout = 0.2\n",
    "optimizer = 'RMSprop'\n",
    "L2 = 0.001\n",
    "\n",
    "model_small_dataset =generate_model(layers_dim, lr, dropout, optimizer, L2)\n",
    "#model_small_dataset.summary()\n",
    "\n",
    "history = model_small_dataset.fit(\n",
    "      train_data,train_labels,\n",
    "      epochs=50,\n",
    "      verbose=2,\n",
    "      #validation_data = (mini_testing_2c9_data, mini_labels_testing_2c9_data)\n",
    "      validation_data = (test_data, test_labels)\n",
    "      )\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"../datasets/CYP/\"\n",
    "\n",
    "shared_data = pd.read_csv(os.path.join(PATH_DATA, \"shared_set_cyp.csv\"))\n",
    "labels_2c9 = (shared_data[\"p450-cyp2c9 Activity Outcome\"] == \"Active\").values.astype(int)\n",
    "labels_3a4 = (shared_data[\"p450-cyp3a4 Activity Outcome\"] == \"Active\").values.astype(int)\n",
    "testing_2c9_data = pd.read_csv(os.path.join(PATH_DATA, \"only_2c9_set_cyp.csv\"))\n",
    "labels_testing_2c9 = (testing_2c9_data[\"p450-cyp2c9 Activity Outcome\"] == \"Active\").values.astype(int)\n",
    "testing_3a4_data = pd.read_csv(os.path.join(PATH_DATA, \"only_3a4_set_cyp.csv\"))\n",
    "labels_testing_3a4 = (testing_3a4_data[\"p450-cyp3a4 Activity Outcome\"] == \"Active\").values.astype(int)\n",
    "\n",
    "features_shared = np.load(os.path.join(\"features\", \"shared_set_features.npy\"))\n",
    "features_only_2c9 = np.load(os.path.join(\"features\", \"only_2c9_set_features.npy\"))\n",
    "features_only_3a4 = np.load(os.path.join(\"features\", \"only_3a4_set_features.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting features (fingerprints)\n",
    "\n",
    "There are many [fingerprints](http://rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf):\n",
    "\n",
    "- Morgan\n",
    "- RDKit fingerprint (to do)\n",
    "- MACCS (to do)\n",
    "\n",
    "Posar link paper que compara fingerprints (Dani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(input_sdf):\n",
    "    structures_shared = Chem.SDMolSupplier(input_sdf)\n",
    "    features = []\n",
    "    for mol in structures_shared:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol,2,nBits=1024)\n",
    "        arr = np.zeros((0,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(fp,arr)\n",
    "        features.append(arr)\n",
    "    return np.array(features)\n",
    "\n",
    "if os.path.exists(os.path.join(\"features\", \"shared_set_features.npy\")):\n",
    "    features_shared = np.load(os.path.join(\"features\", \"shared_set_features.npy\"))\n",
    "else:\n",
    "    features_shared = get_features(os.path.join(PATH_DATA, \"shared_set_cyp.sdf\"))\n",
    "    np.save(os.path.join(\"features\", \"shared_set_features.npy\"), features_shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting physicochemical descriptors\n",
    "\n",
    "Molecular descriptors are widely employed to present molecular characteristics in cheminformatics. Various molecular-descriptor-calculation software programs have been developed.\n",
    "\n",
    "- [Mordred](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0258-y) (To do)\n",
    "- PaDEL - Descriptor\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mordred import Calculator, descriptors\n",
    "\n",
    "# create descriptor calculator with all descriptors\n",
    "calc = Calculator(descriptors, ignore_3D=True)\n",
    "#len(calc.descriptors)\n",
    "\n",
    "df = pd.read_csv('CYP2C9_dataset_training.csv')\n",
    "class_col = df['Class']\n",
    "class_arr = class_col.to_numpy()\n",
    "class_arr = np.reshape(class_arr, [class_arr.shape[0],1])\n",
    "class_arr = np.squeeze(class_arr)\n",
    "smi_col = df['SMILES']\n",
    "smi_arr = smi_col.to_numpy()\n",
    "smi_arr = np.reshape(smi_arr, [smi_arr.shape[0],1])\n",
    "smi_arr = np.squeeze(smi_arr)\n",
    "\n",
    "# calculate descriptors for a single molecule using smile\n",
    "mol = Chem.MolFromSmiles(smi_arr[300])\n",
    "calc(mol)[:3]\n",
    "\n",
    "# get descriptors in a df\n",
    "df_descriptors = calc.pandas(mols_short)\n",
    "#df_descriptors['SLogP']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ferature selection\n",
    "\n",
    "There are mainly [two ways](https://machinelearningmastery.com/feature-selection-with-categorical-data/) of chossing features of a dataset taking into account it's labels (`select_features()`):\n",
    "\n",
    "- [chi square](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) (`chi2`)\n",
    "- [Mutual info classification](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) (`mutual_info_classif`)\n",
    "\n",
    "Also we can remove all features whose variance doesnâ€™t meet some threshold, independently of how are they classified.\n",
    "By default, it removes all zero-variance features, i.e. features that have the same value in all samples (`select_features_threshold`):\n",
    "\n",
    "- [Removing features with low variance](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection): The smaller threshold ($p$), the more restrictive is the function, since the smaller will be the variance threshold:\n",
    "\n",
    "    $Var_{threshold} =  p(1-p)<Var_{x} \\rightarrow Accepted$\n",
    "\n",
    "We can combine both ways to select features `select_features_comb()`:\n",
    "- First we will select the features taking into account the labels of the data, using `select_features()`.\n",
    "- Then we will set a variance threshold to the selected features with `select_features_threshold()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_univar(X_train, Y_train, X_test, score_func=chi2, k_best=None, percentile = None):\n",
    "    \"\"\"score_func=chi2 (default), mutual_info_classif\"\"\"\n",
    "    if not k_best == None:\n",
    "        fs = SelectKBest(score_func=score_func, k=k_best)\n",
    "    elif not percentile == None:\n",
    "        fs = SelectPercentile(score_func=score_func, percentile=percentile)\n",
    "    else:\n",
    "        print(\"Introduce the number of best features to be kept (`k_best`) or the percentil.\")\n",
    "        return\n",
    "    fs.fit(X_train, Y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "    \n",
    "def select_features_threshold(X_train, X_test, threshold=0.9):\n",
    "    \"\"\"It removes all features whose variance doesnâ€™t meet some threshold. It doesn't take into account the \"\"\"\n",
    "    sel = VarianceThreshold(threshold=(threshold * (1 - threshold)))\n",
    "    X_train_threshold = sel.fit_transform(X_train)\n",
    "    X_test_threshold = sel.transform(X_test)\n",
    "    return X_train_threshold, X_test_threshold\n",
    "\n",
    "def select_features_comb(X_train, Y_train, X_test, score_func=chi2, k_best=None, percentile = None, threshold=0.9):\n",
    "    X_train_fs, X_test_fs, _ = select_features_univar(X_train, Y_train, X_test, score_func=chi2, k_best=k_best, percentile = percentile)\n",
    "    X_train_comb, X_test_comb = select_features_threshold(X_train_fs, X_test_fs, threshold=threshold)\n",
    "    return X_train_comb, X_test_comb\n",
    "\n",
    "def plot_score(fs, print_scores=False):\n",
    "    \"\"\"plot the score for all the features\"\"\"\n",
    "    if print_scores:\n",
    "        for i in range(len(fs.scores_)):\n",
    "            print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "    plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
